{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set X: Practicing GPU-Accelerated Python\n",
    "\n",
    "*LSST DSFP Session 21*\n",
    "\n",
    "*Instructor: Lehman Garrison (https://github.com/lgarrison)*\n",
    "\n",
    "Let's revisit some of the problems from Lecture and Problem Set IX and see if we can solve them on the GPU!\n",
    "\n",
    "**Goals**\n",
    "\n",
    "In this problem set, you'll:\n",
    "- convert NumPy code to CuPy\n",
    "- time the results\n",
    "- reason about when GPU code will be faster than CPU code\n",
    "- write a Numba CUDA kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\pi$ with CuPy\n",
    "\n",
    "In Lecture IX, we computed the value of $\\pi$ by generating $N$ random points and measuring number $N_{\\rm circle}$ fell in a circle incribed in a square.  The formula was:\n",
    "\n",
    "$$\n",
    "\\pi = \\frac{4N_{\\rm circle}}{N}.\n",
    "$$\n",
    "\n",
    "We implemented this NumPy version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 64.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numpy_compute_pi(N):\n",
    "    rng = np.random.default_rng(123)\n",
    "    pos = rng.random((2, N))\n",
    "    N_circle = np.sum(np.sum(pos**2, axis=0) < 1)\n",
    "    return 4 * N_circle / N\n",
    "\n",
    "%timeit numpy_compute_pi(1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to turn this function into a CuPy function that runs on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "stream = cp.cuda.get_current_stream()\n",
    "\n",
    "def cupy_compute_pi(N):\n",
    "    # YOUR CODE HERE\n",
    "    rng = cp.random.default_rng(123)\n",
    "    pos = rng.random((2, N))\n",
    "    N_circle = cp.sum(cp.sum(pos**2, axis=0) < 1)\n",
    "    return 4 * N_circle / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And time your GPU code. How does it compare to the NumPy version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 ms ± 219 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cupy_compute_pi(1_000_000)\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be ~10x faster. Monte Carlo algorithms are a good example of methods that work well on the GPU because they are highly parallel and all the \"data\" generation (the random numbers) can happen on the GPU. Only the final result (the measured $\\pi$ value) has to be communicated back to the CPU.\n",
    "\n",
    "This method does still suffer from the same drawback we discussed in Lecture IX, which is using unnecessary temporary memory. In the bonus section, you'll have a chance to implement a smarter method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy in the Sky\n",
    "\n",
    "In Lecture IX, we counted the number of points that fell within angle `theta` of a given `(RA_0, DEC_0)` point in on-sky coordinates. The NumPy version, borrowed from Astropy, looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_count_sep(RA_0, DEC_0, lon, lat, theta):\n",
    "    \"\"\"Based on Astropy's function for computing the angular separation\n",
    "    between two points on the sphere.\n",
    "\n",
    "    License: BSD-3-Clause\n",
    "    \"\"\"\n",
    "\n",
    "    sdlon = np.sin(lon - RA_0)\n",
    "    cdlon = np.cos(lon - RA_0)\n",
    "    slat1 = np.sin(DEC_0)\n",
    "    slat2 = np.sin(lat)\n",
    "    clat1 = np.cos(DEC_0)\n",
    "    clat2 = np.cos(lat)\n",
    "\n",
    "    num1 = clat2 * sdlon\n",
    "    num2 = clat1 * slat2 - slat1 * clat2 * cdlon\n",
    "    denominator = slat1 * slat2 + clat1 * clat2 * cdlon\n",
    "\n",
    "    sep = np.arctan2(np.hypot(num1, num2), denominator)\n",
    "\n",
    "    return np.sum(sep < theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we timed that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 s ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from astropy.table import Table\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "N = 10**7\n",
    "catalog = Table(\n",
    "    {\n",
    "        \"RA\": np.radians(rng.uniform(0, 180, N)),\n",
    "        \"DEC\": np.radians(rng.uniform(-90, 90, N)),\n",
    "    }\n",
    ")\n",
    "\n",
    "RA_0 = catalog[\"RA\"][0]\n",
    "DEC_0 = catalog[\"DEC\"][0]\n",
    "theta = np.radians(10.0)\n",
    "\n",
    "%timeit numpy_count_sep(RA_0, DEC_0, catalog['RA'], catalog['DEC'], theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time this function on the GPU. Let's assume the data is originating on the CPU (as it would in a real application), so you should include the transfer time to and from the GPU.\n",
    "\n",
    "Here's a few prompts to help you think through this:\n",
    "\n",
    "1. Do you need to rewrite the function to use CuPy, or can you use it as-is?\n",
    "1. How can you check that the computation is actually happening on the GPU?\n",
    "1. What function do you use to send data to the GPU? What function do you use to fetch the data back to the CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5 ms ± 209 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "RA_gpu = cp.asarray(catalog['RA'])\n",
    "DEC_gpu = cp.asarray(catalog['DEC'])\n",
    "\n",
    "RA_0 = RA_gpu[0]\n",
    "DEC_0 = DEC_gpu[0]\n",
    "theta = cp.radians(10.0)\n",
    "\n",
    "res = numpy_count_sep(RA_0, DEC_0, RA_gpu, DEC_gpu, theta)\n",
    "res.get()\n",
    "\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be ~25x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy 2PCF\n",
    "\n",
    "In Problem Set IX, we went through 3 implementations of a two-point correlation function:\n",
    "\n",
    "1. a slow version with Python `for` loops,\n",
    "1. a faster version with NumPy array broadcasting,\n",
    "1. an even faster version with Numba `for` loops.\n",
    "\n",
    "CuPy doesn't help us with `for` loops (at least via the NumPy-like interface we've been using), so let's try adapting the NumPy array version to CuPy.\n",
    "\n",
    "Here's what that version looked like (we'll use fake data points this time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1.\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "def numpy_compute_2pcf(pos, R_max, N_bin):\n",
    "    N = len(pos)\n",
    "\n",
    "    delta = pos[:, None] - pos\n",
    "    delta -= L * np.rint(delta / L)  # periodic wrap\n",
    "\n",
    "    d_ij = np.sqrt(np.sum(delta ** 2, axis=-1))\n",
    "\n",
    "    hist_edges = np.linspace(0, R_max, N_bin + 1)\n",
    "    sep_hist = np.histogram(d_ij.flat, bins=hist_edges)[0]\n",
    "\n",
    "    sep_hist[0] -= N  # remove self-pairs\n",
    "\n",
    "    RR = N * (N - 1) / L ** 3 * 4/3 * np.pi * (hist_edges[1:]**3 - hist_edges[:-1]**3)\n",
    "\n",
    "    xi = sep_hist / RR - 1\n",
    "\n",
    "    return xi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time the NumPy version for 2 values of `N`: `50` and `1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 μs ± 1.72 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit pos = rng.uniform(0, L, (50, 3))\n",
    "numpy_compute_2pcf(pos, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.6 ms ± 121 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit pos = rng.uniform(0, L, (1_000, 3))\n",
    "numpy_compute_2pcf(pos, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a CuPy version of this code. Can you call the `numpy_compute_2pcf()` function unmodified with CuPy arrays, as we did in the Monte-Carlo problem above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cupy_compute_2pcf(pos, R_max, N_bin):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    N = len(pos)\n",
    "\n",
    "    delta = pos[:, None] - pos\n",
    "    delta -= L * np.rint(delta / L)  # periodic wrap\n",
    "\n",
    "    d_ij = np.sqrt(np.sum(delta ** 2, axis=-1))\n",
    "\n",
    "    hist_edges = cp.linspace(0, R_max, N_bin + 1)\n",
    "    sep_hist = cp.histogram(d_ij.reshape(-1), bins=hist_edges)[0]\n",
    "\n",
    "    sep_hist[0] -= N  # remove self-pairs\n",
    "\n",
    "    RR = N * (N - 1) / L ** 3 * 4/3 * np.pi * (hist_edges[1:]**3 - hist_edges[:-1]**3)\n",
    "\n",
    "    xi = sep_hist / RR - 1\n",
    "\n",
    "    return xi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 μs ± 2.41 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit pos = rng.uniform(0, L, (50, 3))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pos_g = cp.asarray(pos)\n",
    "\n",
    "cupy_compute_2pcf(pos_g, 0.1, 100).get()\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 ms ± 10.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit pos = rng.uniform(0, L, (1000, 3))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pos_g = cp.asarray(pos)\n",
    "\n",
    "cupy_compute_2pcf(pos_g, 0.1, 100).get()\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results\n",
    "\n",
    "You should find that the NumPy version is faster for `N=50`, and the CuPy version is faster for `N=1000`. Why do you think this is? Let's do a quiz to guide your intuition.\n",
    "\n",
    "**Quiz**\n",
    "\n",
    "What is (1) the time complexity of sending the data to the GPU, and (2) the time complexity of calculating the 2PCF? Think back to what you learned in Problem Set IX about computational complexity.\n",
    "\n",
    "1. $\\mathcal{O}(1)$, $\\mathcal{O}(N)$\n",
    "1. $\\mathcal{O}(1)$, $\\mathcal{O}(N^2)$\n",
    "1. $\\mathcal{O}(N)$, $\\mathcal{O}(N)$\n",
    "1. $\\mathcal{O}(N)$, $\\mathcal{O}(N^2)$\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the answer</summary>\n",
    "\n",
    "It's 4. $\\mathcal{O}(N)$, $\\mathcal{O}(N^2)$! This is hinting at the fact that for small $N$, the data transfer time to the GPU dominates the amount of work to compute the 2PCF, so we're better off staying on the CPU. But in the limit of large $N$, the amount of 2PCF work will always exceed the data transfer work, so it becomes worthwhile to send the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba CUDA: Monte Carlo $\\pi$\n",
    "\n",
    "Let's use Numba CUDA to accelerate our Monte Carlo $\\pi$ computation. Recall how this worked on the CPU in Lecture IX: Numba let us avoid allocating big arrays of random numbers and instead let us generate them on-the-fly.  We'll do the same thing here, except using CUDA kernels.\n",
    "\n",
    "Make each GPU thread generate `N_iter` points using a `for` loop, and keep a count of `N_circle` as you go. To add the results across all threads, allocate an array with length equal to the number of threads, and have each thread write its result there. Then sum the array using CuPy after the kernel.\n",
    "\n",
    "To generate the `x` and `y` points, use this code:\n",
    "\n",
    "```python\n",
    "x = xoroshiro128p_uniform_float64(rng_states, thread_id)\n",
    "y = xoroshiro128p_uniform_float64(rng_states, thread_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float64\n",
    "\n",
    "@cuda.jit\n",
    "def cuda_compute_pi(rng_states, N_iter, out):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    thread_id = cuda.grid(1)\n",
    "\n",
    "    N_circle = 0\n",
    "    for i in range(N_iter):\n",
    "        x = xoroshiro128p_uniform_float64(rng_states, thread_id)\n",
    "        y = xoroshiro128p_uniform_float64(rng_states, thread_id)\n",
    "        if x**2 + y**2 < 1:\n",
    "            N_circle += 1\n",
    "\n",
    "    out[thread_id] = N_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_sample=64000000\n"
     ]
    }
   ],
   "source": [
    "N_iter = 10000\n",
    "threads_per_block = 64\n",
    "blocks = 100\n",
    "\n",
    "N_sample = threads_per_block * blocks * N_iter\n",
    "print(f'{N_sample=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/lgarrison/lsst-dsfp/venv/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3 ms ± 437 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=123)\n",
    "out = cp.zeros(threads_per_block * blocks, dtype=np.int64)\n",
    "\n",
    "cuda_compute_pi[blocks, threads_per_block](rng_states, 10000, out)\n",
    "cuda.synchronize()\n",
    "pi = 4. * out.sum() / N_sample\n",
    "# print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time the CuPy implementation from above. You should find that the Numba CUDA version is 10x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.1 ms ± 441 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cupy_compute_pi(N_sample)\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Idea: Numba CUDA 2PCF\n",
    "\n",
    "It's should be possible to get a big speedup on the 2PCF calculation using Numba CUDA, just like we got a big speedup using Numba instead of NumPy on the CPU. This is certainly trickier than the exercises we've done so far, and the optimal implementation should use shared memory, which we haven't covered. As such, I haven't coded up an answer myself, but I recommend this as a fun challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfp-cuda3",
   "language": "python",
   "name": "dsfp-cuda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
